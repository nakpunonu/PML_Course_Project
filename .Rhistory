testing <- read.csv("./PML_course_project/pml-testing.csv")
# to get an idea of the distribution of the classe variable
table(training$classe)/length(training$classe)
# Prepare data; remove columns that consist of all NAs
# or that is all blank in the testing data
missingcols <- is.na(colSums(testing[,7:ncol(testing)]))
names(missingcols) <- NULL #remove vector names (not necessary, just preference)
out <- !missingcols #columns to be removed now identified as TRUE
testing2 <- testing[,7:ncol(testing)][,out] #remove unwanted
testing2 <- cbind(testing[1:6],testing2) #attach first couple of columns
#remove same columns from training data set
intersect <- names(training) %in% names(testing2) #columns in both
training2 <- training[,intersect] # select columns from training
training2 <- cbind(training2,training[,160]) #add back y variable
names(training2)[60] <- "classe" #add back y variable, name appropriately
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE)
library(caret)
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE)
inTrain
summary(inTrain)
training3 <- training2[inTrain,]
validation3 <- training2[-inTrain,]
modFit <- train(classe ~ .,method="rpart",data=training3)
library(rattle)
fancyRpartPlot(modFit$finalModel)
print(modFit$finalModel)
View(training3)
modFit <- train(classe ~ .,method="rpart",data=training3[,7:60])
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel,main="Tree")
fancyRpartPlot(modFit$finalModel,main="Partition Tree")
print(modFit$finalModel)
predict(modFit,newdata=validation3)
valtest <- predict(modFit,newdata=validation3)
valtest
confusionMatrix(validation3$type,valtest)
confusionMatrix(validation3$classe,valtest)
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60],prox=TRUE)
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60],prox=TRUE)
#read data
training <- read.csv("./PML_course_project/pml-training.csv")
testing <- read.csv("./PML_course_project/pml-testing.csv")
# to get an idea of the distribution of the classe variable
table(training$classe)/length(training$classe)
# Prepare data; remove columns that consist of all NAs
# or that is all blank in the testing data
missingcols <- is.na(colSums(testing[,7:ncol(testing)]))
names(missingcols) <- NULL #remove vector names (not necessary, just preference)
out <- !missingcols #columns to be removed now identified as TRUE
testing2 <- testing[,7:ncol(testing)][,out] #remove unwanted
testing2 <- cbind(testing[1:6],testing2) #attach first couple of columns
#remove same columns from training data set
intersect <- names(training) %in% names(testing2) #columns in both
training2 <- training[,intersect] # select columns from training
training2 <- cbind(training2,training[,160]) #add back y variable
names(training2)[60] <- "classe" #add back y variable, name appropriately
#To get general idea of the data set
summary(training2)
#Segment Training data into Training and Validation Set
library(caret)
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE) #75%
training3 <- training2[inTrain,]
validation3 <- training2[-inTrain,]
#Create Random Forest Model Random Forest
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60],prox=TRUE)
library(randomForest)
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60],prox=TRUE)
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60])
?train
#Create Boost Model
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
install.packages("gbm")
install.packages("gbm")
#Create Boost Model
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
valtest3 <- predict(modFit2,newdata=validation3)
#Create Boost Model
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
valtest3 <- predict(modFit3,newdata=validation3)
#Create Boost Model
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
valtest3 <- predict(modFit3,newdata=validation3)
training
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
#read data
training <- read.csv("./PML_course_project/pml-training.csv")
testing <- read.csv("./PML_course_project/pml-testing.csv")
# to get an idea of the distribution of the classe variable
table(training$classe)/length(training$classe)
# Prepare data; remove columns that consist of all NAs
# or that is all blank in the testing data
missingcols <- is.na(colSums(testing[,7:ncol(testing)]))
names(missingcols) <- NULL #remove vector names (not necessary, just preference)
out <- !missingcols #columns to be removed now identified as TRUE
testing2 <- testing[,7:ncol(testing)][,out] #remove unwanted
testing2 <- cbind(testing[1:6],testing2) #attach first couple of columns
#remove same columns from training data set
intersect <- names(training) %in% names(testing2) #columns in both
training2 <- training[,intersect] # select columns from training
training2 <- cbind(training2,training[,160]) #add back y variable
names(training2)[60] <- "classe" #add back y variable, name appropriately
#To get general idea of the data set
summary(training2)
#Segment Training data into Training and Validation Set
library(caret)
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE) #75%
training3 <- training2[inTrain,]
validation3 <- training2[-inTrain,]
#Create Boost Model
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
library(gbm)
#Create Boost Model
modFit3 <- train(classe ~ .,method="gbm",data=training3[,7:60],verbose=FALSE)
#read data
training <- read.csv("./PML_course_project/pml-training.csv")
testing <- read.csv("./PML_course_project/pml-testing.csv")
# to get an idea of the distribution of the classe variable
table(training$classe)/length(training$classe)
# Prepare data; remove columns that consist of all NAs
# or that is all blank in the testing data
missingcols <- is.na(colSums(testing[,7:ncol(testing)]))
names(missingcols) <- NULL #remove vector names (not necessary, just preference)
out <- !missingcols #columns to be removed now identified as TRUE
testing2 <- testing[,7:ncol(testing)][,out] #remove unwanted
testing2 <- cbind(testing[1:6],testing2) #attach first couple of columns
#remove same columns from training data set
intersect <- names(training) %in% names(testing2) #columns in both
training2 <- training[,intersect] # select columns from training
training2 <- cbind(training2,training[,160]) #add back y variable
names(training2)[60] <- "classe" #add back y variable, name appropriately
#Segment Training data into Training and Validation Set
library(caret)
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE) #75%
training3 <- training2[inTrain,]
validation3 <- training2[-inTrain,]
#Create Partition Tree Model
modFit <- train(classe ~ .,method="rpart",data=training3[,7:60])
library(rattle)
fancyRpartPlot(modFit$finalModel,main="Partition Tree")
print(modFit$finalModel)
valtest <- predict(modFit,newdata=validation3)
#Classification Matrix
confusionMatrix(validation3$classe,valtest)
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60],prox=TRUE)
modFit2 <- train(classe ~ .,method="rf",data=training3[,7:60],prox=TRUE)
confusionMatrix(validation3$classe,valtest2)
#read data
training <- read.csv("./PML_course_project/pml-training.csv")
testing <- read.csv("./PML_course_project/pml-testing.csv")
# to get an idea of the distribution of the classe variable
table(training$classe)/length(training$classe)
# Prepare data; remove columns that consist of all NAs
# or that is all blank in the testing data
missingcols <- is.na(colSums(testing[,7:ncol(testing)]))
names(missingcols) <- NULL #remove vector names (not necessary, just preference)
out <- !missingcols #columns to be removed now identified as TRUE
testing2 <- testing[,7:ncol(testing)][,out] #remove unwanted
testing2 <- cbind(testing[1:6],testing2) #attach first couple of columns
#remove same columns from training data set
intersect <- names(training) %in% names(testing2) #columns in both
training2 <- training[,intersect] # select columns from training
training2 <- cbind(training2,training[,160]) #add back y variable
names(training2)[60] <- "classe" #add back y variable, name appropriately
#To get general idea of the data set
summary(training2)
#Segment Training data into Training and Validation Set
library(caret)
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE) #75%
training3 <- training2[inTrain,]
validation3 <- training2[-inTrain,]
#Create Partition Tree Model
modFit <- train(classe ~ .,method="rpart",data=training3[,7:60])
library(rattle)
fancyRpartPlot(modFit$finalModel,main="Partition Tree")
print(modFit$finalModel)
valtest <- predict(modFit,newdata=validation3)
#Classification Matrix
confusionMatrix(validation3$classe,valtest)
?randomForest
library(randomForest)
?randomForest
randomForest(classe~.,data=training3[,7:60])
testfit <- randomForest(classe~.,data=training3[,7:60])
valtest<- predict(testfit,newdata=validation3)
valtest
confusionMatrix(validation3$classe,valtest)
training
training3
#read data
training <- read.csv("./PML_course_project/pml-training.csv")
testing <- read.csv("./PML_course_project/pml-testing.csv")
# to get an idea of the distribution of the classe variable
table(training$classe)/length(training$classe)
# Prepare data; remove columns that consist of all NAs
# or that is all blank in the testing data
missingcols <- is.na(colSums(testing[,7:ncol(testing)]))
names(missingcols) <- NULL #remove vector names (not necessary, just preference)
out <- !missingcols #columns to be removed now identified as TRUE
testing2 <- testing[,7:ncol(testing)][,out] #remove unwanted
testing2 <- cbind(testing[1:6],testing2) #attach first couple of columns
#remove same columns from training data set
intersect <- names(training) %in% names(testing2) #columns in both
training2 <- training[,intersect] # select columns from training
training2 <- cbind(training2,training[,160]) #add back y variable
names(training2)[60] <- "classe" #add back y variable, name appropriately
#To get general idea of the data set
summary(training2)
#Segment Training data into Training and Validation Set
library(caret)
inTrain <- createDataPartition(y=training2$classe,p=0.7, list=FALSE) #75%
training3 <- training2[inTrain,]
validation3 <- training2[-inTrain,]
#Create Partition Tree Model
modFit <- train(classe ~ .,method="rpart",data=training3[,7:60])
library(rattle)
fancyRpartPlot(modFit$finalModel,main="Partition Tree")
print(modFit$finalModel)
valtest <- predict(modFit,newdata=validation3)
#Classification Matrix
confusionMatrix(validation3$classe,valtest)
#Create Random Forest Model
library(randomForest)
modFit2 <- randomForest(classe ~ .,data=training3[,7:60])
valtest2 <- predict(modFit2,newdata=validation3)
#Classification Matrix2 Random Forest
confusionMatrix(validation3$classe,valtest2)
insample <- predict(modFit2,newdata=training3)
confusionMatrix(training3$classe,insample)
modFit2
?modFit2
?randomForest
modFit2$call
modFit2$type
modFit2$importance
modFit2$ntree
modFit2$confusion
modFit2$test
modFit2$oob.times
modFit2$err.rate
modFit2$forest
modFit2$mtry
modFit2$predicted
confusionMatrix(training3$classe,modFit2$predicted)
Classification Matrix2 Random Forest
confusionMatrix(validation3$classe,valtest2)
confusionMatrix(training3$classe,modFit2$predicted)
?train
modFit$method
modFit$modelType
modFit$bestTune
modFit$call
modFit$finalModel
modFit$perfNames
modFit$Accuracy
modFit$Accu
modFit$times
modFit$yLimits
modFit$results
modFit$metric
predict(modFit,newdata=training3)
test <- predict(modFit,newdata=training3)
confusionMatrix(training3$classe,test)
modFit$results
class(modFit$results)
modFit$results[1,2]
#out-of-sample error rate estimate
confusionMatrix(validation3$classe,valtest)
?confusionMatrix
confusionMatrix$table
confusionMatrix1 <- confusionMatrix(validation3$classe,valtest)
confusionMatrix1$table
confusionMatrix1$positive
confusionMatrix1$overall
class(confusionMatrix1$overall)
confusionMatrix1$overall[1]
confusionMatrix2 <- confusionMatrix(validation3$classe,valtest2)
confusionMatrix2$table
confusionMatrix2$overall[1]
#out-of-sample error rate estimate (together with matrix)
confusionMatrix1 <- confusionMatrix(validation3$classe,valtest)
confusionMatrix1$table
confusionMatrix1$overall[1]
#in-sample error rate estimate
confusionMatrix(training3$classe,modFit2$predicted)
confusionMatrix(training3$classe,modFit2$predicted)$overall
confusionMatrix(training3$classe,modFit2$predicted)$overall[1]
confusionMatrix2$overall[1]
?randomForest
modFit2$importance
modFit2$importance[1:10]
class(modFit2$importance)
head(modFit2$importance,10)
head(modFit2$importance,15)
modFit2$err.rate
modFit2$confusion
testing2
predictions <- predict(modFit2,newdata=testing2)
predictions
class(predictions)
length(predictions)
setwd("./PML_course_project/pml-predictions")
list.files
list.files()
predictions
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictions)
head(modFit2$importance,15) #top 15 important variables
plot(training3)
plot(training3$num_window)
plot(training3$num_window,col="classe")
library(ggplot2)
?seq
seq(1,nrow(training3))
training3index<-seq(1,nrow(training3))
View(training3)
qqplot(training3index,training3$num_window)
qqplot(training3$num_window)
plot(training3$num_window)
qqplot(training3$num_window)
qqplot(training3index,training3$num_window)
qqplot(training3index,training3$num_window,colour=training3$classe)
qqplot(training3index,training3$num_window,color=training3$classe)
qqplot(training3index,training3$num_window,col=training3$classe)
library(ggplot2)
qqplot(training3index,training3$num_window,col=training3$classe)
qplot(training3index,training3$num_window,col=training3$classe)
head(modFit2$importance,15) #top 15 important variables
qplot(training3index,training3$roll_belt,col=training3$classe)
qplot(training3index,training3$pitch_belt,col=training3$classe)
?ggplot
?ggplot2
?ggplot
qplot(training3$pitch_belt,col=training3$classe)
qplot(training3$pitch_belt,fill=training3$classe)
qplot(training3$pitch_belt,col=training3$classe)
qplot(training3index,training3$pitch_belt,col=training3$classe)
qplot(training3index,training3$roll_belt,col=training3$classe)
qplot(training3index,training3$yaw_belt,col=training3$classe)
?rep
rep("A",5)
rep(A,5)
rep(c("A","B","C"),EACH=5)
rep(c("A","B","C"),each=5)
Factor <- rep(c("A","B","C"),each=5)
Factor
times <- rep(c(1,2,3),each=5)
times
play
play <- data.frame(Factor=Factor,Times=times)
play
qplot(Times,col=Factor,data=play)
qplot(Times,col=Factor,fill=play)
qplot(Times,fill=Factor,data=play)
playindex <- seq(1,nrow(play))
playindex
qplot(playindex,play$Times,col=play$Factor)
qplot(training3index,training3$yaw_belt,col=training3$classe)
qplot(training3index,training3$pitch_forearm,col=training3$classe)
qplot(roll_belt,yaw_belt,col=classe,data=training3)
head(modFit2$importance,15)
qplot(num_window,roll_belt,col=classe,data=training3)
qplot(num_window,pitch_belt,col=classe,data=training3)
qplot(num_window,yaw_belt,col=classe,data=training3)
qplot(yaw_belt,num_window,col=classe,data=training3)
qplot(num_window,yaw_belt,col=classe,data=training3)
qplot(training3index,num_window,col=classe,data=training3)
qplot(num_window,fill=classe,data=training3)
qplot(roll_belt,yaw_belt,col=classe,data=training3)
qplot(roll_belt,pitch_belt,col=classe,data=training3)
qplot(yaw_belt,pitch_belt,col=classe,data=training3)
?qplolt
?qplot
iris
head(iris)
qplot(Petal.Width,fill=Species,data=iris)
indexiris <- seq(1,nrow(iris))
indexiris
qplot(indexiris,iris$Petal.Width,col=Species)
qplot(indexiris,iris$Petal.Width,col=iris$Species)
head(modFit2$importance,15)
qplot(num_window,roll_belt,col=classe,data=training3)
explore1 <- qplot(num_window,roll_belt,col=classe,data=training3)
explore2 <- qplot(num_window,pitch_belt,col=classe,data=training3)
grid.arrange(explore1,explore2,ncol=2)
library(gridExtra)
grid.arrange(explore1,explore2,ncol=2)
grid.arrange(explore1,explore2,nrow=2)
grid.arrange(explore1,explore2,ncol=2)
explore3 <- qplot(num_window,yaw_belt,col=classe,data=training3)
explore4 <- qplot(num_window,total_accel_belt,col=classe,data=training3)
grid.arrange(explore1,explore2,explore3,explore4,ncol=2)
?grid.arrange
grid.arrange(explore1,explore2,explore3,explore4,ncol=2,main="Exploratory Graphs")
grid.arrange(explore1,explore2,explore3,explore4,ncol=2,main="Exploratory Graphs (Using the 5 Most Important Predictors)")
grid.arrange(explore1,explore2,explore3,explore4,ncol=2,main="Exploratory Graphs (Using the Model's Top 5 Predictors)")
confusionMatrix2 <- confusionMatrix(validation3$classe,valtest2)
confusionMatrix2$table
confusionMatrix2$overall[1]
confusionMatrix2$overall[1]
1-confusionMatrix2$overall[1]
predictions <- predict(modFit2,newdata=testing2)
predictions
getwd()
setwd("C:/Users/nakpunonu/Google Drive/Courses Online/R Programming/R Working Directory")
getwd()
head(summary(training2))
summary(training2)
class(summary(training2))
?table
summary(training2)[1]
summary(training2)[1:50]
cars
head(cars)
mean(cars$dist)
?knit2html()
confusionMatrix2$overall[1]
confusionMatrix(training3$classe,modFit2$predicted)$overall[1]
1-confusionMatrix(training3$classe,modFit2$predicted)$overall[1]
names(cars)
confusionMatrix(training3$classe,modFit2$predicted)
?knit2html()
knit2html("./PML_course_project","./PML_course_project")
library(knitr)
knit2html("./PML_course_project","./PML_course_project")
knit2html("./PML_course_project","./PML_course_project")
knit2html("./PML_course_project/PML course project write-up.RMD","./PML_course_project")
knit2html("./PML_course_project/PML course project write-up.RMD")
setwd("./PML_course_project/OUTPUT DOC")
knit2html("./PML course project write-up.RMD")
knit2html("./PML course project write-up.RMD")
setwd(".//")
getwd()
sim <- function(){
a <- NULL
for(i in 1:1000){
a <- cbind(a,rexp(40,0.2))
}
avgs <<- colMeans(a)
hist(avgs)
}
sim <- function(){
a <- NULL
for(i in 1:1000){
a <- cbind(a,rexp(40,0.2))
}
avgs <<- colMeans(a)
library(ggplot2)
qplot(avgs)
}
sim()
sim <- function(){
a <- NULL
for(i in 1:1000){
a <- cbind(a,rexp(40,0.2))
}
avgs <<- colMeans(a)
hist(avgs,col=2)
}
sim()
sim <- function(){
a <- NULL
for(i in 1:1000){
a <- cbind(a,rexp(40,0.2))
}
avgs <<- colMeans(a)
hist(avgs,col="lightblue")
}
sim()
a <- NULL
for(i in 1:1000){
a <- cbind(a,rexp(40,0.2))
}
avgs <<- colMeans(a)
hist(avgs,col="lightblue")
avgs
boxplot(avgs)
par(mfrow(c(1,2)))
?jpar
?par
mfrow(par(c(1,2))
mfrow(par(c(1,2)))
par(mfrow = c(1,2))
hist(avgs)
boxplot(avgs)
par(mfrow = c(1,2))
hist(avgs,col="lightblue", main ="Histogram of Simulated Averages",xlab="Averages of 40 observation from Exponential Distribution")
boxplot(avgs, col="lightblue", main="Boxplot of Simulated Averages")
c(mean(avgs) - 1.96 * (1/0.2/(sqrt(40))),1/0.2 + 1.96 * (1/0.2/(sqrt(40))))
c(1/0.2 - 1.96 * (1/0.2/(sqrt(40))),1/0.2 + 1.96 * (1/0.2/(sqrt(40))))
?t.test
qt(0.975)
qt(0.975,df=29)
qt(0.025,df=29)
error <- qt(0.975,df=8)*(30/sqrt(9))
1100+error
1100-error
qt(0.975,df=8)
3*2
/2.306004
6/2.306004
9*0.6+9*0.68
a<-9*0.6+9*0.68
a
a/18
qt(0.975,df=18)
qt(0.95,df=16)
